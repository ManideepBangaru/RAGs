{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RAG (Retrieval-Augmented Generation) System\n",
    "\n",
    "### Overview\n",
    "\n",
    "This code implements a basic Retrieval-Augmented Generation (RAG) system for processing and querying PDF documents. The system encodes the document content into a vector store, which can then be queried to retrieve relevant information.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### PDF processing and text extraction\n",
    "Text chunking for manageable processing\n",
    "Vector store creation using <a href=\"https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/\">FAISS</a> and OpenAI embeddings\n",
    "Retriever setup for querying the processed documents\n",
    "Evaluation of the RAG system\n",
    "Method Details\n",
    "\n",
    "#### Document Preprocessing\n",
    "\n",
    "The PDF is loaded using PyPDFLoader.\n",
    "The text is split into chunks using RecursiveCharacterTextSplitter with specified chunk size and overlap.\n",
    "Text Cleaning\n",
    "\n",
    "A custom function replace_t_with_space is applied to clean the text chunks. This likely addresses specific formatting issues in the PDF.\n",
    "\n",
    "#### Vector Store Creation\n",
    "\n",
    "OpenAI embeddings are used to create vector representations of the text chunks.\n",
    "A FAISS vector store is created from these embeddings for efficient similarity search.\n",
    "Retriever Setup\n",
    "\n",
    "A retriever is configured to fetch the top 2 most relevant chunks for a given query.\n",
    "Encoding Function\n",
    "\n",
    "The encode_pdf function encapsulates the entire process of loading, chunking, cleaning, and encoding the PDF into a vector store.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "Modular Design: The encoding process is encapsulated in a single function for easy reuse.\n",
    "Configurable Chunking: Allows adjustment of chunk size and overlap.\n",
    "Efficient Retrieval: Uses FAISS for fast similarity search.\n",
    "Evaluation: Includes a function to evaluate the RAG system's performance.\n",
    "Usage Example\n",
    "\n",
    "The code includes a test query: \"What is the main cause of climate change?\". This demonstrates how to use the retriever to fetch relevant context from the processed document.\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "The system includes an evaluate_rag function to assess the performance of the retriever, though the specific metrics used are not detailed in the provided code.\n",
    "\n",
    "#### Benefits of this Approach\n",
    "\n",
    "Scalability: Can handle large documents by processing them in chunks.\n",
    "Flexibility: Easy to adjust parameters like chunk size and number of retrieved results.\n",
    "Efficiency: Utilizes FAISS for fast similarity search in high-dimensional spaces.\n",
    "Integration with Advanced NLP: Uses OpenAI embeddings for state-of-the-art text representation.\n",
    "Conclusion\n",
    "\n",
    "This simple RAG system provides a solid foundation for building more complex information retrieval and question-answering systems. By encoding document content into a searchable vector store, it enables efficient retrieval of relevant information in response to queries. This approach is particularly useful for applications requiring quick access to specific information within large documents or document collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/20/3yg6fdqn0q37hmd96kcp0czh0000gn/T/ipykernel_44290/3338928691.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from helper_functions import *\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import *\n",
    "from evaluate_rag import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the pdf document path\n",
    "path = \"data/ManideepResume.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = texts\n",
    "    # cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_vector_store = encode_pdf(path, chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"What are the list of companies ?\"\n",
    "context = retrieve_context_per_question(test_query, chunks_query_retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering the question from the retrieved context...\n",
      "Answering the question from the retrieved context...\n",
      "Answering the question from the retrieved context...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 3 test case(s) in parallel: |██████████|100% (3/3) [Time Taken: 00:15,  5.13s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Correctness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output matches the expected output exactly., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4, reason: None, error: None)\n",
      "  - ❌ Contextual Relevancy (score: 0.0, threshold: 1.0, strict: False, evaluation model: gpt-4, reason: The score is 0.00 because the context focuses on professional experience and job roles, but does not provide any information about a 'profile name'., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the name of the profile\n",
      "  - actual output: Manideep Bangaru\n",
      "  - expected output: Manideep Bangaru\n",
      "  - context: None\n",
      "  - retrieval context: ['ManideepBangaru\\nbmd994@gmail.c om+917416228028\\nOBJECTIVE:SeekingachallengingcareerpositioninanorganizationwhereIcanusemytechnicalskills&creativitytomakeasignificantcontributiontowardsgrowth&developmen toforganizationalongwithmypersonalgrowth\\nProfessionalExperience:\\nCompan yName:GameopediaDataSolutionsPvtLtd,Hyderabad(Sep’22toPresent)Domain:DataScienceToolsUsed:Python,mysql,Confluenc e,JiraShortSummar yoftheprojectsdealingwith:\\n●TeamLead-DataScience:Client:GameopediaRole:TeamLead-DataScienceResponsibilities:1)Directingthedatascienceshipintheorganization2)DirectpointofcontactforanyDataSciencework3)Conflictresolution,Problemsolving4)Workshopsorganizerandprojectingdatasciencecapabilities5)Frequentcatchupwiththeinternal&externalclientstoidentifyopportunities6)DesignedOpportunityassessmentplan7)SmootherexecutionwithreasonabletimelinesusingAgilemethodology8)Crossfunctionalinteractions9)Team-Performanceassessments', 'Compan yName:AffineAnalyticsPvt.ltd,Bangalore(Jun’18toDec’18)Domain:DataScienceToolsUsed:Python,R,SQL,Tableau,ExcelShortSummar yoftheprojectsdealtwith:\\n●TitleDemandForecasting:Client:SonyRole:SeniorBusinessAnalystResponsibilities:1)World’soneofthebestgamingcompanywantstoknowhowthesalesbehaviourofagameisgoingtobeafterfewmonthsofitsrelease2)Sothatitcanmanageitsspendonthegametomaximizethesales3)Designedanendtoendpipelinewhichinitiallylearnsfromthepreviouslyreleasedgamesandtheirrespectivetransactionaldata,andprovidedatemplatetotheclientinwhichheneedstodumpthedataofthepreviousquartersothatthisdatacanbeusedtothrowoutthepredictionsofthenextquarter']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Correctness (GEval) (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output 'Cognizant Technology Solutions, Hyderabad (Jan '19 to Jun '21)' is entirely different from the expected output 'Gameopedia'., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4, reason: None, error: None)\n",
      "  - ❌ Contextual Relevancy (score: 0.0, threshold: 1.0, strict: False, evaluation model: gpt-4, reason: The score is 0.00 because the context discussed roles and responsibilities at Affine Analytics Pvt. Ltd and a project at Cognizant Technology Solutions, but did not provide any information about the 'latest company'., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the latest company?\n",
      "  - actual output: Cognizant Technology Solutions, Hyderabad (Jan '19 to Jun '21)\n",
      "  - expected output: Gameopedia\n",
      "  - context: None\n",
      "  - retrieval context: ['Compan yName:AffineAnalyticsPvt.ltd,Bangalore(Jun’18toDec’18)Domain:DataScienceToolsUsed:Python,R,SQL,Tableau,ExcelShortSummar yoftheprojectsdealtwith:\\n●TitleDemandForecasting:Client:SonyRole:SeniorBusinessAnalystResponsibilities:1)World’soneofthebestgamingcompanywantstoknowhowthesalesbehaviourofagameisgoingtobeafterfewmonthsofitsrelease2)Sothatitcanmanageitsspendonthegametomaximizethesales3)Designedanendtoendpipelinewhichinitiallylearnsfromthepreviouslyreleasedgamesandtheirrespectivetransactionaldata,andprovidedatemplatetotheclientinwhichheneedstodumpthedataofthepreviousquartersothatthisdatacanbeusedtothrowoutthepredictionsofthenextquarter', '●SearchengineusingBM25retriever:Client:MultipleclientsRole:DataScienceSpecialistResponsibilities:1)Ideaistobuildasearchenginewhichreducestheeffortandincreasesthespeedofassociatesinfindingrightdocumentforrightincident2)Thesolutiongoesthroughapipelinewhichundergoesvariouspre-processingstepssuchasworktokenizing,stopwordremoval,lemmatization,documenttermmatrix,etc.3)BM25retrieverisbeingusedtocrawlthroughdocumenttermmatrixandgivesusthebestmatch\\nCompan yName:CognizantTechnologySolutions,Hyderabad(Jan’19toJun’21)Domain:DataScienceToolsUsed:GoogleColab,TensorFlo w,Python,R,MicrosoftPowerBIShortSummar yoftheprojectsdealingwith:']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Correctness (GEval) (score: 0.4402351012147755, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output mentions 'Engineering' which aligns with the expected output, but it provides more detail than required., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4, reason: None, error: None)\n",
      "  - ✅ Contextual Relevancy (score: 1.0, threshold: 1.0, strict: False, evaluation model: gpt-4, reason: The score is 1.00 because the retrieval context perfectly matches the input, providing a clear and relevant answer to the question asked., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the highest qualification ?\n",
      "  - actual output: The highest qualification is a Bachelor of Technology in Electronics and Communications Engineering from Jawaharlal Nehru Technological University, Hyderabad, India.\n",
      "  - expected output: Engineering\n",
      "  - context: None\n",
      "  - retrieval context: ['JawaharlalNehruTechnologic alUniversity ,Hyderabad,IndiaJuly2011toJune2015BachelorofTechnologyinElectronicsandCommunic ationsEngineering(ECE)Percentagesecured:73.35', 'Achievements:\\n●CertifiedBaseSAS9.4Programmer(CertifiedbySASIndia)\\n●CertifiedinINSOFE’SCPEEcertificationranked3rdworldwideondatasciencewith78%aggregat e(CertifiedbyLTIofCarnegieMellonUniversity)\\n●Leadtheteam,whichautomatesSASscriptstoPythontoreducetherevenue\\nSkills:\\nProgramminglanguages:TensorFlo w,Python,R,SQL\\nVisualizationTools:Tableau,MicrosoftPowerBI,GlueViz,Excel\\nConceptualSkills:Huggingfacetransformers,Comput erVision,NaturalLanguageprocessing,MachineLearning\\nEducationalBackground:\\nInternationalSchoolofEngineering(INSOFE),Hyderabad,IndiaApril2016toOct2016\\nwebsite:http://www.insofe.edu.in/CertificatePrograminBigdataAnalyticsandOptimizationItisa6-monthAppliedEngineeringcourseinDataScience,BigdataanalyticsandOptimizationinwhichwewillbeexposedtovariousanalyticaltoolsandMachineLearningmodelstofindthepatternsinthedata,thenusedforpredictionsanddecision-making.TheProgramiscertifiedforthequality,pedagogyandassessmen tbyLTIofCarnegieMellonUniversity .']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness (GEval): 33.33% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Relevancy: 33.33% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. \n",
       "‼️  NOTE: You can also run evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. \n",
       "‼️  NOTE: You can also run evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_rag(chunks_query_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = 'stuff',\n",
    "    retriever = chunks_query_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_chain.run(query = \"Today date is 29th September, 2024. You are a inerview resume analyzer in an IT company, What is the total experience of the candidate ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The candidate's total experience can be calculated by summing up the duration of their professional engagements as listed in their resume.\n",
       "\n",
       "1. **Gameopedia Data Solutions Pvt Ltd, Hyderabad**: Sep '22 to Present (Sep '22 to Sep '24) = 2 years\n",
       "2. **Accenture, Hyderabad**: Jun '21 to Sep '22 = 1 year and 3 months\n",
       "3. **Cognizant Technology Solutions, Hyderabad**: Jan '19 to Jun '21 = 2 years and 6 months\n",
       "4. **Affine Analytics Pvt Ltd, Bangalore**: Jun '18 to Dec '18 = 6 months\n",
       "5. **Nielsen India Pvt Ltd, Bangalore**: Oct '16 to Jun '18 = 1 year and 8 months\n",
       "6. **Deloitte, Hyderabad**: May '16 to Aug '16 = 4 months\n",
       "\n",
       "Summing these up:\n",
       "\n",
       "- 2 years (Gameopedia)\n",
       "- 1 year and 3 months (Accenture)\n",
       "- 2 years and 6 months (Cognizant)\n",
       "- 6 months (Affine)\n",
       "- 1 year and 8 months (Nielsen)\n",
       "- 4 months (Deloitte)\n",
       "\n",
       "Converting all into months for easier summation:\n",
       "- 2 years = 24 months\n",
       "- 1 year and 3 months = 15 months\n",
       "- 2 years and 6 months = 30 months\n",
       "- 6 months = 6 months\n",
       "- 1 year and 8 months = 20 months\n",
       "- 4 months = 4 months\n",
       "\n",
       "Total = 24 + 15 + 30 + 6 + 20 + 4 = 99 months\n",
       "\n",
       "Converting back to years:\n",
       "- 99 months ≈ 8 years and 3 months\n",
       "\n",
       "Therefore, the candidate has approximately 8 years and 3 months of total professional experience."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
